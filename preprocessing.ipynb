{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813c5dca-381d-4fe8-9593-c132244b7b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using rf_grid.best_estimator_ from workspace.\n",
      "Underlying classifier classes_: [1]\n",
      "y_train value counts (current variable):\n",
      "{1: 40000}\n",
      "\n",
      "=== RandomForest (best) evaluation ===\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      5000\n",
      "\n",
      "    accuracy                           1.00      5000\n",
      "   macro avg       1.00      1.00      1.00      5000\n",
      "weighted avg       1.00      1.00      1.00      5000\n",
      "\n",
      "Confusion matrix:\n",
      "[[5000]]\n",
      "ROC AUC: cannot compute (probabilities are constant).\n",
      "\n",
      "=== RandomForest (best) evaluation ===\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     50000\n",
      "           1       0.09      1.00      0.17      5000\n",
      "\n",
      "    accuracy                           0.09     55000\n",
      "   macro avg       0.05      0.50      0.08     55000\n",
      "weighted avg       0.01      0.09      0.02     55000\n",
      "\n",
      "Confusion matrix:\n",
      "[[    0 50000]\n",
      " [    0  5000]]\n",
      "ROC AUC: cannot compute (probabilities are constant).\n",
      "\n",
      "Checking for perfect single-feature predictors on training data (this is expensive for many features):\n",
      "No obvious perfect single-value predictors found (limited check).\n",
      "\n",
      "If you see that the classifier only knows one class (classes_ length == 1), or CV f1 == 1.0:\n",
      " - Check that y_train actually contains both 0 and 1 (print y_train.value_counts()).\n",
      " - Check kernel/data leakage: ensure no feature equals the target or uniquely identifies fraud (e.g., 'Transaction_Status' may directly encode 'Fraud' or 'Reversed').\n",
      " - Remove or mask any features that leak the label (drop features and retry).\n",
      " - Ensure StratifiedKFold was used in GridSearchCV so folds contain positives/negatives.\n",
      " - As a quick fix, reduce the preprocessor to numeric-only and run GridSearch to see if perfect score disappears.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# --- 1) Robust evaluation helper that handles single-column predict_proba ---\n",
    "def safe_predict_proba(pipe, X):\n",
    "    \"\"\"\n",
    "    Return probability of the positive class (1) robustly even if predict_proba returns only 1 column.\n",
    "    \"\"\"\n",
    "    # try normal predict_proba\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        probs = pipe.predict_proba(X)\n",
    "        if probs.ndim == 2 and probs.shape[1] == 2:\n",
    "            # standard binary case: second col is prob for class 1\n",
    "            return probs[:, 1]\n",
    "        elif probs.ndim == 2 and probs.shape[1] == 1:\n",
    "            # only one column present: need to inspect classes_\n",
    "            clf = pipe.named_steps.get('clf', None)\n",
    "            if clf is not None and hasattr(clf, \"classes_\"):\n",
    "                cls = clf.classes_\n",
    "                # if only class is 1 => probability of class1 is 1.0 for all predicted\n",
    "                if len(cls) == 1:\n",
    "                    single_class = cls[0]\n",
    "                    if single_class == 1:\n",
    "                        return np.ones(len(X))\n",
    "                    else:\n",
    "                        return np.zeros(len(X))\n",
    "            # fallback: return the single column as-is (assume it's prob(1))\n",
    "            return probs.ravel()\n",
    "        else:\n",
    "            # unexpected shape\n",
    "            return np.zeros(len(X))\n",
    "    else:\n",
    "        # no predict_proba available\n",
    "        preds = pipe.predict(X)\n",
    "        return preds.astype(float)  # 0/1\n",
    "\n",
    "def robust_eval(pipe, Xv, yv, name=\"model\"):\n",
    "    preds = pipe.predict(Xv)\n",
    "    probs = safe_predict_proba(pipe, Xv)\n",
    "    print(f\"\\n=== {name} evaluation ===\")\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(yv, preds, zero_division=0))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(yv, preds))\n",
    "    # compute ROC AUC only if probabilities vary\n",
    "    if np.unique(probs).shape[0] > 1:\n",
    "        try:\n",
    "            print(\"ROC AUC:\", round(roc_auc_score(yv, probs), 4))\n",
    "        except Exception as e:\n",
    "            print(\"ROC AUC computation failed:\", e)\n",
    "    else:\n",
    "        print(\"ROC AUC: cannot compute (probabilities are constant).\")\n",
    "\n",
    "# --- 2) Inspect the fitted GridSearch model you used (best_rf variable) ---\n",
    "# If you saved the best grid into 'rf_grid' or 'best_rf'\n",
    "# Replace names below if different (e.g., rf_grid, rnd, etc.)\n",
    "try:\n",
    "    # try to retrieve best_rf from current namespace (GridSearch earlier)\n",
    "    best_rf = rf_grid.best_estimator_\n",
    "    print(\"Using rf_grid.best_estimator_ from workspace.\")\n",
    "except Exception:\n",
    "    try:\n",
    "        best_rf = best_rf  # if already defined\n",
    "    except Exception:\n",
    "        best_rf = None\n",
    "\n",
    "if best_rf is None:\n",
    "    print(\"No 'best_rf' found in workspace. Load the tuned model file if saved, e.g.:\")\n",
    "    print(\"best_rf = joblib.load(r'C:\\\\BFSI\\\\fraud_models\\\\random_forest_tuned.pkl')\")\n",
    "else:\n",
    "    # show classes known to the underlying classifier\n",
    "    clf = best_rf.named_steps.get('clf', None)\n",
    "    if clf is not None and hasattr(clf, \"classes_\"):\n",
    "        print(\"Underlying classifier classes_:\", clf.classes_)\n",
    "    else:\n",
    "        print(\"Underlying classifier has no classes_ attribute.\")\n",
    "\n",
    "    # quick check: how many unique labels in training set used by grid?\n",
    "    print(\"y_train value counts (current variable):\")\n",
    "    try:\n",
    "        print(y_train.value_counts().to_dict())\n",
    "    except Exception:\n",
    "        print(\"y_train not present in scope.\")\n",
    "\n",
    "# --- 3) Run robust evaluation on validation and test sets ---\n",
    "# Use the objects X_val, y_val, X_test, y_test that should already be in workspace\n",
    "if 'best_rf' in globals() and best_rf is not None:\n",
    "    robust_eval(best_rf, X_val, y_val, name=\"RandomForest (best)\")\n",
    "    robust_eval(best_rf, X_test, y_test, name=\"RandomForest (best)\")\n",
    "else:\n",
    "    print(\"best_rf not available: skip evaluation. Load model then call robust_eval().\")\n",
    "\n",
    "# --- 4) Debug: check for features that perfectly separate classes (simple rule-based check) ---\n",
    "# This helps detect leakage: if a single feature perfectly separates fraud vs non-fraud,\n",
    "# model can achieve perfect CV F1.\n",
    "def perfect_predictors(X, y, top_n=10):\n",
    "    \"\"\"Return features that perfectly or near-perfectly separate classes.\"\"\"\n",
    "    out = []\n",
    "    for c in X.columns:\n",
    "        try:\n",
    "            # skip numeric floats with many unique values\n",
    "            uniques = X[c].nunique(dropna=False)\n",
    "            if uniques <= 100:  # only test low-to-medium cardinality\n",
    "                grouped = pd.crosstab(X[c], y)\n",
    "                # if any row has only one class and count > 0 that's suspicious\n",
    "                for idx, row in grouped.iterrows():\n",
    "                    if (row == 0).any():\n",
    "                        # compute separation ratio\n",
    "                        total = row.sum()\n",
    "                        max_count = row.max()\n",
    "                        ratio = max_count / total\n",
    "                        if ratio == 1.0:\n",
    "                            out.append((c, idx, total, ratio))\n",
    "        except Exception:\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "print(\"\\nChecking for perfect single-feature predictors on training data (this is expensive for many features):\")\n",
    "try:\n",
    "    pp = perfect_predictors(X_train, y_train)\n",
    "    if pp:\n",
    "        print(\"Found suspicious perfect predictors (feature, value, count, ratio):\")\n",
    "        for i in pp[:20]:\n",
    "            print(i)\n",
    "    else:\n",
    "        print(\"No obvious perfect single-value predictors found (limited check).\")\n",
    "except Exception as e:\n",
    "    print(\"Perfect predictors check failed:\", e)\n",
    "\n",
    "# --- 5) If suspicious perfect score persists: suggestions ---\n",
    "print(\"\"\"\n",
    "If you see that the classifier only knows one class (classes_ length == 1), or CV f1 == 1.0:\n",
    " - Check that y_train actually contains both 0 and 1 (print y_train.value_counts()).\n",
    " - Check kernel/data leakage: ensure no feature equals the target or uniquely identifies fraud (e.g., 'Transaction_Status' may directly encode 'Fraud' or 'Reversed').\n",
    " - Remove or mask any features that leak the label (drop features and retry).\n",
    " - Ensure StratifiedKFold was used in GridSearchCV so folds contain positives/negatives.\n",
    " - As a quick fix, reduce the preprocessor to numeric-only and run GridSearch to see if perfect score disappears.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3057d8b9-ca2b-4b7f-a954-fb35fc838409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial split sizes (trainval/test): (90000, 19) (10000, 19)\n",
      "Fraud counts (trainval/test): 50000 0\n",
      "Classes present in trainval: [np.int64(0), np.int64(1)]\n",
      "\n",
      "Final split counts:\n",
      "Train size / positive count: (80000, 14) 44444\n",
      "Val   size / positive count: (10000, 14) 5556\n",
      "Test  size / positive count: (10000, 14) 0\n",
      "Train classes present: [np.int64(0), np.int64(1)]\n",
      "Val classes present: [np.int64(0), np.int64(1)]\n",
      "Test classes present: [np.int64(0)]\n",
      "\n",
      "Saved new splits with suffix '_resplit.csv' to: C:\\BFSI\\model_splits\n"
     ]
    }
   ],
   "source": [
    "# Robust resplit and quick check cell\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BASE = Path(r\"C:\\BFSI\")\n",
    "SPLIT_DIR = BASE / \"model_splits\"\n",
    "SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure df is present and timestamp exists\n",
    "if 'df' not in globals():\n",
    "    df_path = BASE / \"card_fraud.csv\"\n",
    "    if not df_path.exists():\n",
    "        raise RuntimeError(\"df not in memory and card_fraud.csv not found at C:\\\\BFSI.\")\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "if \"Transaction_Timestamp\" not in df.columns:\n",
    "    # try to create it\n",
    "    if \"Transaction_Date\" in df.columns and \"Transaction_Time\" in df.columns:\n",
    "        df[\"Transaction_Timestamp\"] = pd.to_datetime(df[\"Transaction_Date\"].astype(str) + \" \" + df[\"Transaction_Time\"].astype(str), errors=\"coerce\")\n",
    "    elif \"Transaction_Date\" in df.columns:\n",
    "        df[\"Transaction_Timestamp\"] = pd.to_datetime(df[\"Transaction_Date\"], errors=\"coerce\")\n",
    "    else:\n",
    "        candidates = [c for c in df.columns if any(k in c.lower() for k in [\"timestamp\",\"datetime\",\"date_time\",\"time\"])]\n",
    "        parsed = False\n",
    "        for c in candidates:\n",
    "            df[\"Transaction_Timestamp\"] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "            if df[\"Transaction_Timestamp\"].notna().sum()>0:\n",
    "                parsed = True\n",
    "                break\n",
    "        if not parsed:\n",
    "            raise RuntimeError(\"Cannot parse Transaction_Timestamp. Fix timestamp column first.\")\n",
    "\n",
    "if \"isFraud\" not in df.columns:\n",
    "    raise RuntimeError(\"Target column isFraud not found in df.\")\n",
    "\n",
    "# Sort by timestamp\n",
    "df_sorted = df.sort_values(\"Transaction_Timestamp\").reset_index(drop=True)\n",
    "\n",
    "# Make temporal test (last 10% by time)\n",
    "cut_test = df_sorted[\"Transaction_Timestamp\"].quantile(0.90)\n",
    "test_df = df_sorted[df_sorted[\"Transaction_Timestamp\"] > cut_test].copy()\n",
    "trainval_df = df_sorted[df_sorted[\"Transaction_Timestamp\"] <= cut_test].copy()\n",
    "\n",
    "print(\"Initial split sizes (trainval/test):\", trainval_df.shape, test_df.shape)\n",
    "print(\"Fraud counts (trainval/test):\", trainval_df[\"isFraud\"].sum(), test_df[\"isFraud\"].sum())\n",
    "\n",
    "# If test set has zero positives (rare), we will fallback to stratified full-split later.\n",
    "# Now try stratified train/val on trainval_df\n",
    "y_tv = trainval_df[\"isFraud\"].astype(int)\n",
    "unique_classes = sorted(y_tv.unique())\n",
    "print(\"Classes present in trainval:\", unique_classes)\n",
    "\n",
    "# compute val fraction relative to trainval to get ~10% of original as val: val_frac_rel = 0.10/0.90\n",
    "val_frac_rel = 0.10 / 0.90\n",
    "\n",
    "# Helper to perform and validate stratified split\n",
    "def try_stratified_split(df_source):\n",
    "    X_tv = df_source.drop(columns=[\"isFraud\"], errors=\"ignore\")\n",
    "    y_tv = df_source[\"isFraud\"].astype(int)\n",
    "    # drop id columns for splitting to avoid using them as features (but keep them not required)\n",
    "    id_cols = [c for c in [\"Transaction_ID\",\"User_ID\",\"Merchant_ID\",\"Device_ID\"] if c in X_tv.columns]\n",
    "    if id_cols:\n",
    "        X_tv_for_split = X_tv.drop(columns=id_cols)\n",
    "    else:\n",
    "        X_tv_for_split = X_tv.copy()\n",
    "    # If either class missing, cannot stratify\n",
    "    if len(y_tv.unique()) < 2:\n",
    "        return None, None, None, None\n",
    "    X_tr, X_v, y_tr, y_v = train_test_split(\n",
    "        X_tv_for_split, y_tv, test_size=val_frac_rel, stratify=y_tv, random_state=42\n",
    "    )\n",
    "    return X_tr, X_v, y_tr, y_v\n",
    "\n",
    "# Try stratified on trainval_df\n",
    "X_train, X_val, y_train, y_val = try_stratified_split(trainval_df)\n",
    "\n",
    "if X_train is None:\n",
    "    print(\"Stratified split on temporal trainval failed (only one class present). Falling back to global stratified split.\")\n",
    "    # Fallback: do stratified split over entire df (sacrifices strict temporal holdout)\n",
    "    X_all = df_sorted.drop(columns=[c for c in [\"Transaction_ID\",\"User_ID\",\"Merchant_ID\",\"Device_ID\",\"isFraud\"] if c in df_sorted.columns], errors=\"ignore\")\n",
    "    y_all = df_sorted[\"isFraud\"].astype(int)\n",
    "    if len(y_all.unique()) < 2:\n",
    "        raise RuntimeError(\"Entire dataset contains only one class; cannot train a classifier.\")\n",
    "    # Primary: create test as before if it has both classes, else stratified on whole dataset\n",
    "    if len(test_df[\"isFraud\"].unique()) > 1:\n",
    "        # keep temporal test, but to get trainval we will take stratified sample from entire dataset excluding test indices\n",
    "        trainval_indices = df_sorted.index[df_sorted[\"Transaction_Timestamp\"] <= cut_test].tolist()\n",
    "        X_trainval = df_sorted.loc[trainval_indices].drop(columns=[c for c in [\"Transaction_ID\",\"User_ID\",\"Merchant_ID\",\"Device_ID\",\"isFraud\"] if c in df_sorted.columns], errors=\"ignore\")\n",
    "        y_trainval = df_sorted.loc[trainval_indices,\"isFraud\"].astype(int)\n",
    "        # now stratified split on these\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=val_frac_rel, stratify=y_trainval, random_state=42)\n",
    "    else:\n",
    "        # test doesn't have both classes; do full stratified split\n",
    "        X_train, X_test2, y_train, y_test2 = train_test_split(X_all, y_all, test_size=0.20, stratify=y_all, random_state=42)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.111111, stratify=y_train, random_state=42)\n",
    "        # overwrite test_df with stratified test (not temporal)\n",
    "        X_test = X_test2\n",
    "        y_test = y_test2\n",
    "        print(\"Fallback used: fully stratified split across all data (non-temporal).\")\n",
    "else:\n",
    "    # we got stratified splits from trainval_df, now set test as temporal test\n",
    "    # Build X_test dropping ids and target\n",
    "    id_cols_test = [c for c in [\"Transaction_ID\",\"User_ID\",\"Merchant_ID\",\"Device_ID\"] if c in test_df.columns]\n",
    "    if id_cols_test:\n",
    "        X_test = test_df.drop(columns=id_cols_test + [\"isFraud\"], errors=\"ignore\")\n",
    "    else:\n",
    "        X_test = test_df.drop(columns=[\"isFraud\"], errors=\"ignore\")\n",
    "    y_test = test_df[\"isFraud\"].astype(int)\n",
    "\n",
    "# Final sanity checks\n",
    "print(\"\\nFinal split counts:\")\n",
    "print(\"Train size / positive count:\", X_train.shape, int(y_train.sum()))\n",
    "print(\"Val   size / positive count:\", X_val.shape, int(y_val.sum()))\n",
    "print(\"Test  size / positive count:\", X_test.shape, int(y_test.sum()))\n",
    "print(\"Train classes present:\", sorted(y_train.unique()))\n",
    "print(\"Val classes present:\", sorted(y_val.unique()))\n",
    "print(\"Test classes present:\", sorted(y_test.unique()))\n",
    "\n",
    "if len(y_train.unique()) < 2 or len(y_val.unique()) < 2:\n",
    "    print(\"\\nWARNING: One of train/val still contains only a single class. Consider using full stratified split or adjusting cutoffs.\")\n",
    "else:\n",
    "    # Save CSVs\n",
    "    X_train.to_csv(SPLIT_DIR / \"X_train_resplit.csv\", index=False)\n",
    "    y_train.to_csv(SPLIT_DIR / \"y_train_resplit.csv\", index=False)\n",
    "    X_val.to_csv(SPLIT_DIR / \"X_val_resplit.csv\", index=False)\n",
    "    y_val.to_csv(SPLIT_DIR / \"y_val_resplit.csv\", index=False)\n",
    "    X_test.to_csv(SPLIT_DIR / \"X_test_resplit.csv\", index=False)\n",
    "    y_test.to_csv(SPLIT_DIR / \"y_test_resplit.csv\", index=False)\n",
    "    print(\"\\nSaved new splits with suffix '_resplit.csv' to:\", SPLIT_DIR)\n",
    "\n",
    "# If you want me to immediately run training with these new splits, say 'train now' and I'll provide the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa26c9d5-0fee-4e6b-b907-4e4a871ef47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frauds: 50000. Target min_frauds in test: 500\n",
      "Found cutoff at quantile 0.4900 -> test frauds: 1000, test size: 51000\n",
      "Trainval shape: (49000, 19) Test shape: (51000, 19) Test frauds: 1000\n",
      "Trainval has only one class after cutoff; falling back to full stratified split.\n",
      "Used full-stratified fallback. Sizes: (71111, 14) (8889, 14) (20000, 14)\n",
      "Saved adjusted splits to: C:\\BFSI\\model_splits\n",
      "\n",
      "Num cols: ['Transaction_Amount', 'Previous_Transaction_Count', 'Distance_Between_Transactions_km', 'Time_Since_Last_Transaction_min', 'Transaction_Velocity', 'txn_hour']\n",
      "Cat cols: ['Transaction_Location', 'Card_Type', 'Transaction_Currency', 'Transaction_Status', 'Authentication_Method', 'Transaction_Category', 'txn_dayofweek']\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RF tuning done in 1.57 minutes.\n",
      "Best RF params: {'clf__max_depth': None, 'clf__min_samples_leaf': 2, 'clf__n_estimators': 100} Best CV f1: 0.5000739125871099\n",
      "\n",
      "=== RandomForest (tuned) VALIDATION ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.50      0.50      4445\n",
      "           1       0.51      0.51      0.51      4444\n",
      "\n",
      "    accuracy                           0.51      8889\n",
      "   macro avg       0.51      0.51      0.51      8889\n",
      "weighted avg       0.51      0.51      0.51      8889\n",
      "\n",
      "Val ROC AUC: 0.507\n",
      "\n",
      "=== RandomForest (tuned) TEST ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.49      0.50     10000\n",
      "           1       0.50      0.50      0.50     10000\n",
      "\n",
      "    accuracy                           0.50     20000\n",
      "   macro avg       0.50      0.50      0.50     20000\n",
      "weighted avg       0.50      0.50      0.50     20000\n",
      "\n",
      "Test ROC AUC: 0.496\n",
      "Saved tuned RF and RF CV results.\n",
      "LightGBM not available or failed to tune: No module named 'lightgbm'\n",
      "Done. Models saved to: C:\\BFSI\\fraud_models Grid results to: C:\\BFSI\\model_splits\n"
     ]
    }
   ],
   "source": [
    "# Auto-adjust temporal test to include frauds & run compact hyperparameter tuning\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import joblib, time, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE = Path(r\"C:\\BFSI\")\n",
    "SPLIT_DIR = BASE / \"model_splits\"\n",
    "MODEL_OUT = BASE / \"fraud_models\"\n",
    "SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load or ensure df\n",
    "if 'df' not in globals():\n",
    "    df_path = BASE / \"card_fraud.csv\"\n",
    "    if not df_path.exists():\n",
    "        raise RuntimeError(\"df not in memory and card_fraud.csv not found at C:\\\\BFSI.\")\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "# Ensure timestamp exists\n",
    "if \"Transaction_Timestamp\" not in df.columns:\n",
    "    if \"Transaction_Date\" in df.columns and \"Transaction_Time\" in df.columns:\n",
    "        df[\"Transaction_Timestamp\"] = pd.to_datetime(df[\"Transaction_Date\"].astype(str) + \" \" + df[\"Transaction_Time\"].astype(str), errors=\"coerce\")\n",
    "    elif \"Transaction_Date\" in df.columns:\n",
    "        df[\"Transaction_Timestamp\"] = pd.to_datetime(df[\"Transaction_Date\"], errors=\"coerce\")\n",
    "    else:\n",
    "        candidates = [c for c in df.columns if any(k in c.lower() for k in [\"timestamp\",\"datetime\",\"date_time\",\"time\"])]\n",
    "        parsed=False\n",
    "        for c in candidates:\n",
    "            df[\"Transaction_Timestamp\"] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "            if df[\"Transaction_Timestamp\"].notna().sum()>0:\n",
    "                parsed=True\n",
    "                break\n",
    "        if not parsed:\n",
    "            raise RuntimeError(\"Cannot parse Transaction_Timestamp.\")\n",
    "\n",
    "if \"isFraud\" not in df.columns:\n",
    "    raise RuntimeError(\"Target 'isFraud' missing in df.\")\n",
    "\n",
    "# Sort dataset by timestamp\n",
    "df_sorted = df.sort_values(\"Transaction_Timestamp\").reset_index(drop=True)\n",
    "\n",
    "# Compute total frauds and desired minimum in test\n",
    "total_frauds = int(df_sorted[\"isFraud\"].sum())\n",
    "min_frauds = max(10, max(1, int(0.01 * total_frauds)))  # at least 10 or 1% of frauds (whichever larger)\n",
    "print(f\"Total frauds: {total_frauds}. Target min_frauds in test: {min_frauds}\")\n",
    "\n",
    "# Start from original 90%/10% cutoff and move cutoff earlier until test contains >= min_frauds positives\n",
    "start_cut = df_sorted[\"Transaction_Timestamp\"].quantile(0.90)\n",
    "cut = start_cut\n",
    "found = False\n",
    "\n",
    "# We'll move the cutoff in steps by decreasing quantile in small increments\n",
    "quantile = 0.90\n",
    "while quantile > 0.01:\n",
    "    cut = df_sorted[\"Transaction_Timestamp\"].quantile(quantile)\n",
    "    test_df = df_sorted[df_sorted[\"Transaction_Timestamp\"] > cut]\n",
    "    n_pos = int(test_df[\"isFraud\"].sum())\n",
    "    if n_pos >= min_frauds:\n",
    "        found = True\n",
    "        print(f\"Found cutoff at quantile {quantile:.4f} -> test frauds: {n_pos}, test size: {len(test_df)}\")\n",
    "        break\n",
    "    quantile -= 0.01  # step 1% earlier\n",
    "if not found:\n",
    "    # as a last resort, include everything after the earliest timestamp that yields at least one fraud\n",
    "    pos_indices = df_sorted.index[df_sorted[\"isFraud\"]==1].tolist()\n",
    "    if pos_indices:\n",
    "        # set cutoff to the timestamp just before the (total_frauds - min_frauds + 1)-th fraud if possible\n",
    "        idx = max(0, pos_indices[-min_frauds]-1) if len(pos_indices) >= min_frauds else pos_indices[0]-1\n",
    "        cut = df_sorted.loc[idx, \"Transaction_Timestamp\"] if idx >= 0 else df_sorted[\"Transaction_Timestamp\"].min()\n",
    "        test_df = df_sorted[df_sorted[\"Transaction_Timestamp\"] > cut]\n",
    "        print(f\"No quantile found; using fallback cut -> test frauds: {int(test_df['isFraud'].sum())}, test size: {len(test_df)}\")\n",
    "    else:\n",
    "        # no frauds at all\n",
    "        print(\"No frauds in dataset at all; can't create a positive test set. Doing stratified full split.\")\n",
    "        # fallback to stratified full split below\n",
    "        found = False\n",
    "\n",
    "# If found True, create trainval from earlier rows and stratify train/val within it\n",
    "if found:\n",
    "    trainval_df = df_sorted[df_sorted[\"Transaction_Timestamp\"] <= cut].copy()\n",
    "    print(\"Trainval shape:\", trainval_df.shape, \"Test shape:\", test_df.shape, \"Test frauds:\", int(test_df['isFraud'].sum()))\n",
    "    # Now stratify train/val\n",
    "    val_frac_rel = 0.10 / 0.90\n",
    "    # Drop ids for stratify split\n",
    "    id_cols = [c for c in [\"Transaction_ID\",\"User_ID\",\"Merchant_ID\",\"Device_ID\"] if c in trainval_df.columns]\n",
    "    X_tv = trainval_df.drop(columns=id_cols + [\"isFraud\"], errors=\"ignore\") if id_cols else trainval_df.drop(columns=[\"isFraud\"])\n",
    "    y_tv = trainval_df[\"isFraud\"].astype(int)\n",
    "    if len(y_tv.unique()) < 2:\n",
    "        print(\"Trainval has only one class after cutoff; falling back to full stratified split.\")\n",
    "        found = False\n",
    "\n",
    "if not found:\n",
    "    # Full stratified split fallback (non-temporal)\n",
    "    X_all = df_sorted.drop(columns=[c for c in [\"Transaction_ID\",\"User_ID\",\"Merchant_ID\",\"Device_ID\",\"isFraud\"] if c in df_sorted.columns], errors=\"ignore\")\n",
    "    y_all = df_sorted[\"isFraud\"].astype(int)\n",
    "    if len(y_all.unique()) < 2:\n",
    "        raise RuntimeError(\"Entire dataset contains only one class; cannot train.\")\n",
    "    # Do stratified 80/10/10\n",
    "    X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_all, y_all, test_size=0.20, stratify=y_all, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.111111, stratify=y_train_all, random_state=42)\n",
    "    X_test = X_test_all\n",
    "    y_test = y_test_all\n",
    "    print(\"Used full-stratified fallback. Sizes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "else:\n",
    "    # perform stratified split on X_tv, y_tv\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_tv, y_tv, test_size=val_frac_rel, stratify=y_tv, random_state=42)\n",
    "    # prepare X_test,y_test from test_df (drop ids and target)\n",
    "    id_cols_test = [c for c in [\"Transaction_ID\",\"User_ID\",\"Merchant_ID\",\"Device_ID\"] if c in test_df.columns]\n",
    "    X_test = test_df.drop(columns=id_cols_test + [\"isFraud\"], errors=\"ignore\") if id_cols_test else test_df.drop(columns=[\"isFraud\"], errors=\"ignore\")\n",
    "    y_test = test_df[\"isFraud\"].astype(int)\n",
    "    print(\"Final sizes after adjustment: X_train:\", X_train.shape, \"X_val:\", X_val.shape, \"X_test:\", X_test.shape)\n",
    "    print(\"Fraud counts (train/val/test):\", int(y_train.sum()), int(y_val.sum()), int(y_test.sum()))\n",
    "\n",
    "# Save adjusted splits\n",
    "X_train.to_csv(SPLIT_DIR / \"X_train_adjusted.csv\", index=False)\n",
    "y_train.to_csv(SPLIT_DIR / \"y_train_adjusted.csv\", index=False)\n",
    "X_val.to_csv(SPLIT_DIR / \"X_val_adjusted.csv\", index=False)\n",
    "y_val.to_csv(SPLIT_DIR / \"y_val_adjusted.csv\", index=False)\n",
    "X_test.to_csv(SPLIT_DIR / \"X_test_adjusted.csv\", index=False)\n",
    "y_test.to_csv(SPLIT_DIR / \"y_test_adjusted.csv\", index=False)\n",
    "print(\"Saved adjusted splits to:\", SPLIT_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# Compact hyperparameter tuning for RandomForest & LightGBM (smaller grid for speed)\n",
    "# -------------------------\n",
    "# Recompute numeric & categorical columns (drop timestamp/date/time columns if present)\n",
    "for D in (X_train, X_val, X_test):\n",
    "    if \"Transaction_Timestamp\" in D.columns:\n",
    "        D[\"Transaction_Timestamp\"] = pd.to_datetime(D[\"Transaction_Timestamp\"], errors=\"coerce\")\n",
    "        D[\"txn_hour\"] = D[\"Transaction_Timestamp\"].dt.hour.fillna(-1).astype(int)\n",
    "        D[\"txn_dayofweek\"] = D[\"Transaction_Timestamp\"].dt.day_name().fillna(\"Unknown\")\n",
    "        D.drop(columns=[\"Transaction_Timestamp\"], inplace=True)\n",
    "    if \"Transaction_Date\" in D.columns:\n",
    "        D.drop(columns=[\"Transaction_Date\"], inplace=True)\n",
    "    if \"Transaction_Time\" in D.columns:\n",
    "        D.drop(columns=[\"Transaction_Time\"], inplace=True)\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "print(\"\\nNum cols:\", num_cols)\n",
    "print(\"Cat cols:\", cat_cols)\n",
    "\n",
    "numeric_transform = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "categorical_transform = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
    "preprocessor = ColumnTransformer(transformers=[(\"num\", numeric_transform, num_cols), (\"cat\", categorical_transform, cat_cols)], remainder=\"drop\")\n",
    "\n",
    "# RandomForest compact grid\n",
    "rf_pipeline = Pipeline([(\"pre\", preprocessor), (\"clf\", RandomForestClassifier(class_weight=\"balanced\", random_state=42, n_jobs=-1))])\n",
    "rf_param_grid_small = {\n",
    "    \"clf__n_estimators\": [100, 200],\n",
    "    \"clf__max_depth\": [10, None],\n",
    "    \"clf__min_samples_leaf\": [1, 2],\n",
    "}\n",
    "rf_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rf_grid = GridSearchCV(rf_pipeline, rf_param_grid_small, scoring=\"f1\", cv=rf_cv, n_jobs=-1, verbose=2, refit=True)\n",
    "t0 = time.time()\n",
    "rf_grid.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "print(f\"RF tuning done in {(t1-t0)/60:.2f} minutes.\")\n",
    "best_rf = rf_grid.best_estimator_\n",
    "print(\"Best RF params:\", rf_grid.best_params_, \"Best CV f1:\", rf_grid.best_score_)\n",
    "\n",
    "# Evaluate best RF\n",
    "def safe_probs(pipe, X):\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        p = pipe.predict_proba(X)\n",
    "        if p.ndim==2 and p.shape[1]==2:\n",
    "            return p[:,1]\n",
    "        elif p.ndim==2 and p.shape[1]==1:\n",
    "            # single-class case\n",
    "            return np.full(len(X), p.ravel()[0])\n",
    "        else:\n",
    "            return np.zeros(len(X))\n",
    "    else:\n",
    "        return pipe.predict(X).astype(float)\n",
    "\n",
    "print(\"\\n=== RandomForest (tuned) VALIDATION ===\")\n",
    "preds_val = best_rf.predict(X_val)\n",
    "print(classification_report(y_val, preds_val, zero_division=0))\n",
    "probs_val = safe_probs(best_rf, X_val)\n",
    "if np.unique(probs_val).shape[0] > 1:\n",
    "    print(\"Val ROC AUC:\", round(roc_auc_score(y_val, probs_val),4))\n",
    "else:\n",
    "    print(\"Val ROC AUC: constant probabilities (cannot compute)\")\n",
    "\n",
    "print(\"\\n=== RandomForest (tuned) TEST ===\")\n",
    "preds_test = best_rf.predict(X_test)\n",
    "print(classification_report(y_test, preds_test, zero_division=0))\n",
    "probs_test = safe_probs(best_rf, X_test)\n",
    "if np.unique(probs_test).shape[0] > 1:\n",
    "    print(\"Test ROC AUC:\", round(roc_auc_score(y_test, probs_test),4))\n",
    "else:\n",
    "    print(\"Test ROC AUC: constant probabilities (cannot compute)\")\n",
    "\n",
    "joblib.dump(best_rf, MODEL_OUT / \"random_forest_tuned_adjusted.pkl\")\n",
    "pd.DataFrame(rf_grid.cv_results_).to_csv(SPLIT_DIR / \"rf_grid_results_adjusted.csv\", index=False)\n",
    "print(\"Saved tuned RF and RF CV results.\")\n",
    "\n",
    "# LightGBM tuning small grid (if available)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    from lightgbm import LGBMClassifier\n",
    "    lgb_pipeline = Pipeline([(\"pre\", preprocessor), (\"clf\", LGBMClassifier(class_weight=\"balanced\", random_state=42))])\n",
    "    lgb_param_grid_small = {\n",
    "        \"clf__n_estimators\": [200, 400],\n",
    "        \"clf__num_leaves\": [31, 63],\n",
    "        \"clf__learning_rate\": [0.05, 0.1],\n",
    "    }\n",
    "    lgb_grid = GridSearchCV(lgb_pipeline, lgb_param_grid_small, scoring=\"f1\", cv=rf_cv, n_jobs=-1, verbose=2, refit=True)\n",
    "    t0 = time.time()\n",
    "    lgb_grid.fit(X_train, y_train)\n",
    "    t1 = time.time()\n",
    "    print(f\"LGB tuning done in {(t1-t0)/60:.2f} minutes.\")\n",
    "    best_lgb = lgb_grid.best_estimator_\n",
    "    print(\"Best LGB params:\", lgb_grid.best_params_, \"Best CV f1:\", lgb_grid.best_score_)\n",
    "    # Evaluate\n",
    "    print(\"\\n=== LightGBM (tuned) VALIDATION ===\")\n",
    "    print(classification_report(y_val, best_lgb.predict(X_val), zero_division=0))\n",
    "    print(\"=== LightGBM (tuned) TEST ===\")\n",
    "    print(classification_report(y_test, best_lgb.predict(X_test), zero_division=0))\n",
    "    joblib.dump(best_lgb, MODEL_OUT / \"lightgbm_tuned_adjusted.pkl\")\n",
    "    pd.DataFrame(lgb_grid.cv_results_).to_csv(SPLIT_DIR / \"lgb_grid_results_adjusted.csv\", index=False)\n",
    "except Exception as e:\n",
    "    print(\"LightGBM not available or failed to tune:\", e)\n",
    "\n",
    "# Save summary\n",
    "summary = [{\"model\":\"random_forest\",\"best_cv_f1\":rf_grid.best_score_,\"best_params\":rf_grid.best_params_}]\n",
    "if 'lgb_grid' in globals():\n",
    "    summary.append({\"model\":\"lightgbm\",\"best_cv_f1\":lgb_grid.best_score_,\"best_params\":lgb_grid.best_params_})\n",
    "pd.DataFrame(summary).to_csv(SPLIT_DIR / \"grid_search_summary_adjusted.csv\", index=False)\n",
    "print(\"Done. Models saved to:\", MODEL_OUT, \"Grid results to:\", SPLIT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebce5ed6-d44f-44cb-b2b4-31e1cf8a2f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF best params: {'clf__max_depth': None, 'clf__min_samples_leaf': 2, 'clf__n_estimators': 100}\n",
      "RF best CV f1: 0.5000739125871099\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'txn_dayofweek', 'txn_hour'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m y_test = pd.read_csv(SPLIT_DIR + \u001b[33m\"\u001b[39m\u001b[33m/y_test_adjusted.csv\u001b[39m\u001b[33m\"\u001b[39m).squeeze().astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Evaluate with default threshold 0.5\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m preds_val = best_rf.predict(X_val)\n\u001b[32m     30\u001b[39m preds_test = best_rf.predict(X_test)\n\u001b[32m     31\u001b[39m probs_val = safe_prob(best_rf, X_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:786\u001b[39m, in \u001b[36mPipeline.predict\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m         Xt = transform.transform(Xt)\n\u001b[32m    787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m].predict(Xt, **params)\n\u001b[32m    789\u001b[39m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = f(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1085\u001b[39m, in \u001b[36mColumnTransformer.transform\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m   1083\u001b[39m     diff = all_names - \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[32m   1084\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1087\u001b[39m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[32m   1088\u001b[39m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[32m   1089\u001b[39m     _check_n_features(\u001b[38;5;28mself\u001b[39m, X, reset=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: columns are missing: {'txn_dayofweek', 'txn_hour'}"
     ]
    }
   ],
   "source": [
    "# Inspect rf_grid and evaluate best estimator (robustly)\n",
    "import joblib, numpy as np, pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, f1_score\n",
    "\n",
    "# rf_grid should exist after GridSearchCV finished\n",
    "best_rf = rf_grid.best_estimator_\n",
    "print(\"RF best params:\", rf_grid.best_params_)\n",
    "print(\"RF best CV f1:\", rf_grid.best_score_)\n",
    "\n",
    "# helper to get prob of class 1 robustly\n",
    "def safe_prob(pipe, X):\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        p = pipe.predict_proba(X)\n",
    "        if p.ndim == 2 and p.shape[1] == 2:\n",
    "            return p[:,1]\n",
    "        elif p.ndim == 2 and p.shape[1] == 1:\n",
    "            # only one class learned; return constant\n",
    "            return np.full(len(X), p.ravel()[0])\n",
    "    return pipe.predict(X).astype(float)\n",
    "\n",
    "# load adjusted splits (if not in memory)\n",
    "SPLIT_DIR = r\"C:\\BFSI\\model_splits\"\n",
    "X_val = pd.read_csv(SPLIT_DIR + \"/X_val_adjusted.csv\")\n",
    "y_val = pd.read_csv(SPLIT_DIR + \"/y_val_adjusted.csv\").squeeze().astype(int)\n",
    "X_test = pd.read_csv(SPLIT_DIR + \"/X_test_adjusted.csv\")\n",
    "y_test = pd.read_csv(SPLIT_DIR + \"/y_test_adjusted.csv\").squeeze().astype(int)\n",
    "\n",
    "# Evaluate with default threshold 0.5\n",
    "preds_val = best_rf.predict(X_val)\n",
    "preds_test = best_rf.predict(X_test)\n",
    "probs_val = safe_prob(best_rf, X_val)\n",
    "probs_test = safe_prob(best_rf, X_test)\n",
    "\n",
    "print(\"Validation report (threshold 0.5):\")\n",
    "print(classification_report(y_val, preds_val, zero_division=0))\n",
    "if np.unique(probs_val).size>1:\n",
    "    try: print(\"Val ROC AUC:\", round(roc_auc_score(y_val, probs_val),4))\n",
    "    except: pass\n",
    "\n",
    "print(\"Test report (threshold 0.5):\")\n",
    "print(classification_report(y_test, preds_test, zero_division=0))\n",
    "if np.unique(probs_test).size>1:\n",
    "    try: print(\"Test ROC AUC:\", round(roc_auc_score(y_test, probs_test),4))\n",
    "    except: pass\n",
    "\n",
    "# Save the grid results summary\n",
    "pd.DataFrame(rf_grid.cv_results_).to_csv(SPLIT_DIR + \"/rf_grid_results_final.csv\", index=False)\n",
    "joblib.dump(best_rf, r\"C:\\BFSI\\fraud_models\\random_forest_tuned_final.pkl\")\n",
    "print(\"Saved best RF pipeline to C:\\\\BFSI\\\\fraud_models\\\\random_forest_tuned_final.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a63466-c1a3-4c86-9b0a-4a5007405e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
